{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b888148",
   "metadata": {},
   "source": [
    "**Disclaimer: Air Quality Data is very local and depends on the exact station location. If we want to have reliable and actionable results, we need to do things carefully. What we do today is not careful, but for demonstration purposes.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbacb499",
   "metadata": {},
   "source": [
    "[Census Data for Pasadena, Compton, Pomona](https://www.census.gov/quickfacts/fact/table/pasadenacitycalifornia,comptoncitycalifornia,pomonacitycalifornia/PST045222)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78850516",
   "metadata": {},
   "source": [
    "## Reading in the Data \n",
    "\n",
    "We first use pandas to read in the data. \n",
    "\n",
    "Remember the directory structure:\n",
    "\n",
    "```\n",
    "- W5_1_AirQuality-|\n",
    "                  |- code |- analysingAQData.ipnyb\n",
    "                  |\n",
    "                  |\n",
    "                  |- data |- EPA_AQI_Compton_1302_20200101_20201231.csv\n",
    "                          |- EPA_AQI_Pasadena_2005_20200101_20201231.csv\n",
    "                          |- EPA_AQI_Pomona_1701_20200101_20201231.csv\n",
    "```\n",
    "\n",
    "This means we have to read 3 files in the data directory. We can either do this manually by reading each file into a named dataframe. \n",
    "\n",
    "When using `pd.read_csv()` we want to make sure that this is read as a time series with the times on the index. So we need to use the `parse_dates` and `index_col` parameters on the column containing the dates called `DateTime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# insert the correct path to the file in the line below\n",
    "aqs_Compton = pd.read_csv(    ,\n",
    "                #  Make sure the dates import in datetime format. We tell pandas that this is a date and not text. \n",
    "                parse_dates = ['DateTime'],\n",
    "                #  Set DATE as the index so you can subset data by time period\n",
    "                index_col = ['DateTime']\n",
    "                  )\n",
    "aqs_Compton\n",
    "\n",
    "# insert the correct path to the file in the line below\n",
    "aqs_Pasadena = pd.read_csv(    ,\n",
    "                #  Make sure the dates import in datetime format. We tell pandas that this is a date and not text. \n",
    "                parse_dates = ['DateTime'],\n",
    "                #  Set DATE as the index so you can subset data by time period\n",
    "                index_col = ['DateTime']\n",
    "                  )\n",
    "aqs_Pasadena\n",
    "\n",
    "# insert the correct path to the file in the line below\n",
    "aqs_Pomona = pd.read_csv(  ,\n",
    "                #  Make sure the dates import in datetime format. We tell pandas that this is a date and not text. \n",
    "                parse_dates = ['DateTime'],\n",
    "                #  Set DATE as the index so you can subset data by time period\n",
    "                index_col = ['DateTime']\n",
    "                  )\n",
    "aqs_Pomona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff1769",
   "metadata": {},
   "source": [
    "However imagine having many files. Reading these in manually does not sound like a good idea. \n",
    "\n",
    "We can use a loop to make things easier. \n",
    "\n",
    "There is a tool called `glob` that can be used to find files in a directory using wildcard characters. For example, try out what `glob.glob(../data/*.csv')` will do. What does the `*` do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15123411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# try out glob one the line below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06c451",
   "metadata": {},
   "source": [
    "Here is a neat way of reading many data files at the same time. We are loading them into a Python Dictionary. \n",
    "\n",
    "Dictionaries are data containers in which data can be looked up with a key. Empty dictionaries are created like this: `DictName = {}`. It is then possible to read and assign the data for each city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71238bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/*.csv')\n",
    "\n",
    "DataDict = {}\n",
    "for i, file in enumerate(files):\n",
    "    cityName= file.split('_')[2] # This extracts the city name from the file name by splitting the string at _\n",
    "    print(cityName)\n",
    "    DataDict[cityName]= pd.read_csv(file,\n",
    "                    #  Make sure the dates import in datetime format. We tell pandas that this is a date and not text. \n",
    "                    parse_dates = ['DateTime'],\n",
    "                    #  Set DATE as the index so you can subset data by time period\n",
    "                    index_col = ['DateTime'])\n",
    "\n",
    "DataDict['Pomona']\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bfc91",
   "metadata": {},
   "source": [
    "## Exploratory Analysis \n",
    "\n",
    "For now let's focus on a single city: Compton. \n",
    "\n",
    "Apply the `.columns` attribute to the `aqs_Compton` data frame to see all the column names.  \n",
    "\n",
    "**Can you guess what each of them is?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01338a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the line below to see the column names \n",
    "aqs_Compton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137c5dd0",
   "metadata": {},
   "source": [
    "Let's look at some important columns. A good way of doing so is using the `.unique()` method, returns a list of unique values. For example we can apply this to the `['parameter']` column to see that we have _NO~2~_, _Ozone_, and _PM2.5_ data in our data frame. We better separate these out later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5894894",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aqs_Compton['parameter'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e0a9a",
   "metadata": {},
   "source": [
    "Let's look at some more columns. Apply `.unique()` to the `['qualifier']` and `['sample_duration']` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete two lines below to apply the .unique() method to ['qualifier'] and ['sample_duration']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b2bdc",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<h3 class=\"alert-heading\">Questions</h3>\n",
    "\n",
    "Look at the printed output. \n",
    "    \n",
    "What do these mean?\n",
    "\n",
    "What does this mean for our analysis? \n",
    "\n",
    "</div>\n",
    "\n",
    "Some data are marked. Let' have a look: \n",
    "\n",
    "A nice thing about pandas is that we can select data on a condition. You have already learned how to select data by time using `.loc[]`. Remember that `aqs_Compton.loc['2020-07-01']` will only show data from July 1st. \n",
    "\n",
    "The `.loc[]` selector can also be used for conditions. For example, if we only want data for the period, where fireworks happened we can specify that we want all the data for which `aqs_compton['qualifier'] == 'IH - Fireworks.'` **(Note the double == sign for a logical comparison)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d259e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqs_Compton[['parameter','sample_measurement','units_of_measure']].loc[aqs_compton['qualifier'] == 'IH - Fireworks.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8f442",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<h3 class=\"alert-heading\">Questions</h3>\n",
    "\n",
    "Look at the printed output. \n",
    "    \n",
    "What do we learn from this? \n",
    "\n",
    "</div>\n",
    "\n",
    "Let's find out which dates and parameters are affected by wildfire smoke? \n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"warning\">\n",
    "<h3 class=\"alert-heading\">Challenge</h3>\n",
    "\n",
    "Find the data that is tagged as affected by wildfires!\n",
    "(Hint: Look at the code above and the string for wildfires)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0419a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below for the challenge  (1 line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8bb9fe",
   "metadata": {},
   "source": [
    "### Quick Summary\n",
    "\n",
    "Let's summarize what we have learned so far. \n",
    "\n",
    "- We have one csv/ dataframe for each station. \n",
    "- The Compton dataframe contains PM2.5, Ozone, and NO~2~ data. \n",
    "    - Is this true for all stations? \n",
    "- Some data are hourly, some are daily (24h). \n",
    "- Some data are flagged for quality, and special events. \n",
    "    - This means we should remove data for periods that are flagges as not valid. \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Our next steps, depend on our research question. \n",
    "\n",
    "For now, let's assume that we want to compare air quality data for the 3 cities and we focus on Ozone for now. \n",
    "\n",
    "Then it would probably be a good idea, to get the ozone data for all 3 cities into a new data frame called `ozoneData`. \n",
    "\n",
    "We know that the actual measurement is stored in the `['sample_measurement']` column and that we can use the `.loc[]` selector to only select data for which the `['parameter']` column contains the label `Ozone`. \n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"warning\">\n",
    "<h3 class=\"alert-heading\">Challenge</h3>\n",
    "\n",
    "Can you complete the code below to do just that?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozoneData = pd.DataFrame() # This creates a new dataframe. \n",
    "# Complete the line below with the correct .loc[] selector\n",
    "ozoneData['Compton']=aqs_Compton['sample_measurement']\n",
    "# Add two more lines below to do the same for Pasadena and Pomona. \n",
    "\n",
    "\n",
    "ozoneData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9e11a",
   "metadata": {},
   "source": [
    "We can now try to compare the 3 stations. Let's start getting the statistics using `.describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb18320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the .describe to the dataframe with ozone data. \n",
    "ozoneData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4f8ca",
   "metadata": {},
   "source": [
    "A plots might also help. Because line plots, don't show much, it is better to aggregate the data into a box plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fa999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code below to create box-plots for the 3 stations. Extra cheers for labels etc.  \n",
    "ozoneData.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9a9e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<h3 class=\"alert-heading\">Questions</h3>\n",
    "\n",
    "How would you interpret this plot?\n",
    "\n",
    "</div>\n",
    "\n",
    "If we assing another column with values for month, we can also use the box plot functionality to investigate the annual cycle of ozone by separating the data `by='month'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozoneData['month']=ozoneData.index.month\n",
    "ozoneData.plot(kind='box', by='month')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea3b81",
   "metadata": {},
   "source": [
    "## Calculating Air Quality Index\n",
    "\n",
    "Remember the [Air Quality Index](https://www.airnow.gov/aqi/) from your classes? \n",
    "\n",
    "It turns out that calculating the AQI is a bit more complicated than I remember and [every country uses their own scale](https://en.wikipedia.org/wiki/Air_quality_index). In the U.S., the EPA calculates the AQI for each criteria air pollutant using a [table of thresholds](https://www.airnow.gov/sites/default/files/2020-05/aqi-technical-assistance-document-sept2018.pdf) for each category and then uses the highes criteria air pollutant AQI as the total AQI.  \n",
    "\n",
    "For the sake of simplicity, lets define our own ISAT 420 AQI. In our case, let's use a simple scaling based on the [National Ambient Air Quality Standards](https://www.epa.gov/criteria-air-pollutants/naaqs-table). We define each component AQI as\n",
    "\n",
    "$AQI = \\frac{\\text{Pollutant Concentration}}{\\text{Reference Standard}} \\times 100$\n",
    "\n",
    "We use the following reference standards.\n",
    "\n",
    "Ozone(1hr)  = 0.124 ppm\n",
    "\n",
    "PM2.5(24hr) = 35.4 $\\mu g\\,m^{-3}$\n",
    "\n",
    "NO2(1hr) = 100 ppb\n",
    "\n",
    "\n",
    "Our workflow will be as follows. We will\n",
    "\n",
    "- focus on Compton, because it has data for NO2, PM2.5, and Ozone. \n",
    "    - We need to confirm that our units are the same \n",
    "- caclulate each component AQI\n",
    "- create a new dataframe with daily resolution that contains all component AQIs\n",
    "- determine AQI as the maximum \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3de734",
   "metadata": {},
   "outputs": [],
   "source": [
    "ozoneStandard = 0.124\n",
    "pmStandard = 35.4\n",
    "NO2Standard = 100\n",
    "\n",
    "# We are now calculating the AQI for ozone. Complete the code below following the AQI equation. \n",
    "aqi_ozone = aqs_Compton['sample_measurement'].loc[aqs_Compton['parameter']=='Ozone']\n",
    "\n",
    "# Add two more lines below to calculate aqi for pm2.5 and NO2\n",
    "aqi_pm = \n",
    "aqi_no2 = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33cf631",
   "metadata": {},
   "source": [
    "We should note that ozone and NO2 are hourly AQIs and PM2.5 is sampled daily. So we should resample all data to daily. This will also take care with periods of missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdfa1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aqi_ozone.head())\n",
    "print(aqi_pm.head())\n",
    "print(aqi_no2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7746ee00",
   "metadata": {},
   "source": [
    "Let's create our new aqi dataframe with each data resampled to 1D. `.resample('1D').mean()` would average each value to get a daily value. It is probably a better idea to take the daily max instead `.max()`.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"warning\">\n",
    "<h3 class=\"alert-heading\">Challenge</h3>\n",
    "\n",
    "Complete the code below to create our aggregated AQI Dataframe\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_Compton = pd.DataFrame()\n",
    "# Modify the line below to resample to 1D using the maximum value in each day. \n",
    "aqi_Compton['Ozone']= aqi_ozone.resample('1D').mean()\n",
    "# Complete two lines below to add PM2.5 and NO2\n",
    "aqi_Compton['PM']= \n",
    "aqi_Compton['NO2']= \n",
    "aqi_Compton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366c638",
   "metadata": {},
   "source": [
    "We can then find the maximum value of each row and assing this to a new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_Compton['AQI_Total']=aqi_Compton.max(axis=1)\n",
    "aqi_Compton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22031b26",
   "metadata": {},
   "source": [
    "Plotting this reveals, which variable is most responsible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_Compton.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3895d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 50, 100, 150, 200, 300, 500]\n",
    "labels = ['Good','Moderate','Unhealthy for Sensitive Groups', 'Unhealthy', 'Very Unhealthy','Hazardous']\n",
    "aqi_Compton['AQI_Class']=pd.cut(aqi_Compton['AQI_Total'], bins, labels= labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_Compton['AQI_Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279cafa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
